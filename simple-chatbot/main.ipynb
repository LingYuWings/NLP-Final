{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Justi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import load_model\n",
    "# Download required NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WordNet Lemmatizer for word normalization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Load intents from JSON file\n",
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tags from intents\n",
    "tags = []\n",
    "for entry in intents['intents']:\n",
    "    tags.append(entry['tag'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lists to hold words, classes, and training data\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_letters = ['?','!','.',',']\n",
    "\n",
    "# Tokenize patterns from intents and preprocess data\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)  # Use extend to add words into word_list\n",
    "        documents.append((word_list, intent['tag']))\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize words and remove ignored characters\n",
    "words = [lemmatizer.lemmatize(word) for word in words if word not in ignore_letters]\n",
    "words = sorted(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = sorted(set(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(words, open('words.pkl','wb'))\n",
    "pickle.dump(classes, open('classes.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training data\n",
    "training = []\n",
    "output_empty = [0] * len(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for document in documents:\n",
    "    bag = []\n",
    "    word_patterns = document[0]\n",
    "    word_patterns = [lemmatizer.lemmatize(word.lower()) for word in word_patterns]\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_patterns else bag.append(0)\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(document[1])] = 1\n",
    "    training.append([bag, output_row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justi\\AppData\\Local\\Temp\\ipykernel_10340\\4024136173.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  training = np.array(training)\n"
     ]
    }
   ],
   "source": [
    "# Shuffle and convert training data to numpy array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the neural network model using Keras\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(classes), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure SGD optimizer\n",
    "sgd = legacy.SGD(learning_rate=0.001,decay = 1e-6,momentum=0.9,nesterov=True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = sgd, metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "model.fit(np.array(train_x),np.array(train_y), epochs = 500, batch_size = 2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot_model.model\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: chatbot_model.model\\assets\n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('chatbot_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "intents = json.loads(open('intents.json').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pickle.load(open('words.pkl','rb'))\n",
    "classes = pickle.load(open('classes.pkl','rb'))\n",
    "model = load_model('chatbot_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bot running start\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "I hope she's alright.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Just wondering.\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Interesting background.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Greetings! What can I do for you?\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Curious question.\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "Let's work together.\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "While I don't have personal opinions, emerging technologies like AI and machine learning have transformative potential.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "Boosting creativity involves exploring new experiences, engaging in brainstorming sessions, and stepping out of your comfort zone.\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "You got it! If you need anything else, feel free to ask.\n"
     ]
    }
   ],
   "source": [
    "def clean_up_sentence(sentence):\n",
    "    sentence_words = nltk.wordpunct_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bag_of_words(sentence):\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)\n",
    "    for w in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == w:\n",
    "                bag[i] = 1\n",
    "                \n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence):\n",
    "    bow = bag_of_words(sentence)\n",
    "    res = model.predict(np.array([bow]))[0]\n",
    "    ERROR_THRESHOLD=0.25\n",
    "    result = [[i,r] for i,r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    \n",
    "    result.sort(key = lambda x:x[1], reverse= True)\n",
    "    return_list = []\n",
    "    for r in result:\n",
    "        return_list.append({'intent':classes[r[0]],'probability':str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def get_response(intents_list, intents_json):\n",
    "    tag = intents_list[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "print(\"bot running start\")\n",
    "\n",
    "while True:\n",
    "    message = input(\"\")\n",
    "    ints = predict_class(message)\n",
    "    res = get_response(ints, intents)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 11ms/step\n",
      "1/1 [==============================] - 0s 12ms/step\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Justi\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3441: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from PyQt5.QtWidgets import QApplication, QMainWindow, QTextBrowser, QLineEdit, QPushButton, QVBoxLayout, QWidget\n",
    "\n",
    "filename = 'intents.json'\n",
    "\n",
    "class ChatBotGUI(QMainWindow):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initUI()\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.intents = json.loads(open(filename).read())\n",
    "\n",
    "        self.words = pickle.load(open('words.pkl', 'rb'))\n",
    "        self.classes = pickle.load(open('classes.pkl', 'rb'))\n",
    "        self.model = load_model('chatbot_model.model')\n",
    "\n",
    "    def initUI(self):\n",
    "        self.setGeometry(100, 100, 400, 600)\n",
    "        self.setWindowTitle('ChatBot GUI')\n",
    "\n",
    "        self.text_browser = QTextBrowser(self)\n",
    "        self.user_input = QLineEdit(self)\n",
    "        self.send_button = QPushButton('Send', self)\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "        layout.addWidget(self.text_browser)\n",
    "        layout.addWidget(self.user_input)\n",
    "        layout.addWidget(self.send_button)\n",
    "\n",
    "        container = QWidget()\n",
    "        container.setLayout(layout)\n",
    "        self.setCentralWidget(container)\n",
    "\n",
    "        self.send_button.clicked.connect(self.on_send_button_click)\n",
    "\n",
    "        # Apply styles using QSS\n",
    "        self.setStyleSheet(\"\"\"\n",
    "            QMainWindow {\n",
    "                background-color: #f0f0f0;\n",
    "            }\n",
    "            QTextBrowser {\n",
    "                background-color: white;\n",
    "                border: 1px solid #ccc;\n",
    "                border-radius: 5px;\n",
    "                padding: 10px;\n",
    "            }\n",
    "            QLineEdit {\n",
    "                background-color: white;\n",
    "                border: 1px solid #ccc;\n",
    "                border-radius: 5px;\n",
    "                padding: 5px;\n",
    "            }\n",
    "            QPushButton {\n",
    "                background-color: #007BFF;\n",
    "                color: white;\n",
    "                border: none;\n",
    "                border-radius: 5px;\n",
    "                padding: 5px 10px;\n",
    "            }\n",
    "            QPushButton:hover {\n",
    "                background-color: #0056b3;\n",
    "            }\n",
    "        \"\"\")\n",
    "\n",
    "    def clean_up_sentence(self, sentence):\n",
    "        sentence_words = nltk.wordpunct_tokenize(sentence)\n",
    "        sentence_words = [self.lemmatizer.lemmatize(word) for word in sentence_words]\n",
    "        return sentence_words\n",
    "\n",
    "    def bag_of_words(self, sentence):\n",
    "        sentence_words = self.clean_up_sentence(sentence)\n",
    "        bag = [0] * len(self.words)\n",
    "        for w in sentence_words:\n",
    "            for i, word in enumerate(self.words):\n",
    "                if word == w:\n",
    "                    bag[i] = 1\n",
    "        return np.array(bag)\n",
    "\n",
    "    def predict_class(self, sentence):\n",
    "        bow = self.bag_of_words(sentence)\n",
    "        res = self.model.predict(np.array([bow]))[0]\n",
    "        ERROR_THRESHOLD = 0.25\n",
    "        result = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "\n",
    "        result.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "        for r in result:\n",
    "            return_list.append({'intent': self.classes[r[0]], 'probability': str(r[1])})\n",
    "        return return_list\n",
    "\n",
    "    def get_response(self, intents_list, intents_json):\n",
    "        tag = intents_list[0]['intent']\n",
    "        list_of_intents = intents_json['intents']\n",
    "        for i in list_of_intents:\n",
    "            if i['tag'] == tag:\n",
    "                result = random.choice(i['responses'])\n",
    "                break\n",
    "        return result\n",
    "\n",
    "    def on_send_button_click(self):\n",
    "        user_message = self.user_input.text()\n",
    "        self.user_input.clear()\n",
    "\n",
    "        ints = self.predict_class(user_message)\n",
    "        response = self.get_response(ints, self.intents)\n",
    "\n",
    "        self.text_browser.append(\"<span style='color: blue;'>You: </span>\" + user_message)\n",
    "        self.text_browser.append(\"<span style='color: green;'>Bot: </span>\" + response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app = QApplication(sys.argv)\n",
    "    gui = ChatBotGUI()\n",
    "    gui.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
